{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RIDESHARE PRICE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_color_codes(\"pastel\")\n",
    "plt.rcParams[\"figure.figsize\"] = [20, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RIDESHARE DATA ON RCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "Found 10 items\n",
      "-rw-r--r--   3 mechols mechols        1708 2019-05-17 14:10 /user/mechols/data/chicago_community_names.csv\n",
      "-rw-r--r--   3 mechols mechols  1804333782 2019-05-16 21:03 /user/mechols/data/chicago_crimes.csv\n",
      "-rw-r--r--   3 mechols mechols       83421 2019-05-24 20:20 /user/mechols/data/december_weather.csv\n",
      "-rw-r--r--   3 mechols mechols       75982 2019-05-24 20:20 /user/mechols/data/february_weather.csv\n",
      "-rw-r--r--   3 mechols mechols   208276005 2019-04-30 20:04 /user/mechols/data/food-inspections.csv\n",
      "-rw-r--r--   3 mechols mechols       85301 2019-05-24 20:20 /user/mechols/data/january_weather.csv\n",
      "-rw-r--r--   3 mechols mechols       82985 2019-05-24 20:20 /user/mechols/data/march_weather.csv\n",
      "-rw-r--r--   3 mechols mechols       72336 2019-05-24 22:45 /user/mechols/data/march_weatherUpdated.csv\n",
      "-rw-r--r--   3 mechols mechols       83817 2019-05-24 20:20 /user/mechols/data/november_weather.csv\n",
      "-rw-r--r--   3 mechols mechols 11980344386 2019-05-21 14:25 /user/mechols/data/rows.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/mechols/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('RideShare').getOrCreate()\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '15g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','20g')])\n",
    "df = spark.read.csv(\"/user/mechols/data/rows.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data consists of +45m rows and 21 columns. \n",
    "* % of non-null = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip ID: string (nullable = true)\n",
      " |-- Trip Start Timestamp: string (nullable = true)\n",
      " |-- Trip End Timestamp: string (nullable = true)\n",
      " |-- Trip Seconds: integer (nullable = true)\n",
      " |-- Trip Miles: double (nullable = true)\n",
      " |-- Pickup Census Tract: long (nullable = true)\n",
      " |-- Dropoff Census Tract: long (nullable = true)\n",
      " |-- Pickup Community Area: integer (nullable = true)\n",
      " |-- Dropoff Community Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tip: integer (nullable = true)\n",
      " |-- Additional Charges: double (nullable = true)\n",
      " |-- Trip Total: double (nullable = true)\n",
      " |-- Shared Trip Authorized: boolean (nullable = true)\n",
      " |-- Trips Pooled: integer (nullable = true)\n",
      " |-- Pickup Centroid Latitude: double (nullable = true)\n",
      " |-- Pickup Centroid Longitude: double (nullable = true)\n",
      " |-- Pickup Centroid Location: string (nullable = true)\n",
      " |-- Dropoff Centroid Latitude: double (nullable = true)\n",
      " |-- Dropoff Centroid Longitude: double (nullable = true)\n",
      " |-- Dropoff Centroid Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().toPandas().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45338599"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scatter Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "numeric_features = [t[0] for t in df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = df.select(numeric_features).sample(False, 0.8).toPandas()\n",
    "axs = pd.scatter_matrix(sampled_data, figsize=(10, 10))\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Correlation to Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "for i in house_df.columns:\n",
    "    if not( isinstance(df.select(i).take(1)[0][0], six.string_types)):\n",
    "        print( \"Correlation to Fare for \", i, df.stat.corr('Fare',i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NULL VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Majority of data filled in. Census Tract at ~70% , field should be dropped\n",
    "* Do we think we will use lat / long data or mostly community area? If just community, we should drop all the lat, long data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Trip ID: bigint, Trip Start Timestamp: bigint, Trip End Timestamp: bigint, Trip Seconds: bigint, Trip Miles: bigint, Pickup Community Area: bigint, Dropoff Community Area: bigint, Fare: bigint, Tip: bigint, Additional Charges: bigint, Trip Total: bigint, Shared Trip Authorized: bigint, Trips Pooled: bigint, Pickup Centroid Latitude: bigint, Pickup Centroid Longitude: bigint, Pickup Centroid Location: bigint, Dropoff Centroid Latitude: bigint, Dropoff Centroid Longitude: bigint, Dropoff Centroid Location: bigint, Pickup Community Area Filled: bigint]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, isnan, lit, sum\n",
    "\n",
    "def count_not_null(c, nan_as_null=False):\n",
    "    \"\"\"Use conversion between boolean and integer\n",
    "    - False -> 0\n",
    "    - True ->  1\n",
    "    \"\"\"\n",
    "    pred = col(c).isNotNull() & (~isnan(c) if nan_as_null else lit(True))\n",
    "    return sum(pred.cast(\"integer\")).alias(c)\n",
    "\n",
    "df.agg(*[count_not_null(c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exprs = [(count_not_null(c) / count(\"*\")).alias(c) for c in df.columns]\n",
    "NonNullData = df.agg(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Trip ID</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Start Timestamp</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip End Timestamp</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Seconds</th>\n",
       "      <td>0.999924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Miles</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Community Area</th>\n",
       "      <td>0.941524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Community Area</th>\n",
       "      <td>0.934505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tip</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Additional Charges</th>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trip Total</th>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shared Trip Authorized</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trips Pooled</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Latitude</th>\n",
       "      <td>0.942170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Longitude</th>\n",
       "      <td>0.942170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Centroid Location</th>\n",
       "      <td>0.942170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Latitude</th>\n",
       "      <td>0.935141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Longitude</th>\n",
       "      <td>0.935141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropoff Centroid Location</th>\n",
       "      <td>0.935141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pickup Community Area Filled</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     0\n",
       "Trip ID                       1.000000\n",
       "Trip Start Timestamp          1.000000\n",
       "Trip End Timestamp            1.000000\n",
       "Trip Seconds                  0.999924\n",
       "Trip Miles                    1.000000\n",
       "Pickup Community Area         0.941524\n",
       "Dropoff Community Area        0.934505\n",
       "Fare                          0.999997\n",
       "Tip                           1.000000\n",
       "Additional Charges            0.999997\n",
       "Trip Total                    0.999997\n",
       "Shared Trip Authorized        1.000000\n",
       "Trips Pooled                  1.000000\n",
       "Pickup Centroid Latitude      0.942170\n",
       "Pickup Centroid Longitude     0.942170\n",
       "Pickup Centroid Location      0.942170\n",
       "Dropoff Centroid Latitude     0.935141\n",
       "Dropoff Centroid Longitude    0.935141\n",
       "Dropoff Centroid Location     0.935141\n",
       "Pickup Community Area Filled  1.000000"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NonNull = NonNullData.toPandas()\n",
    "PercentFilled = NonNull.T\n",
    "PercentFilled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Pickup Census Tract\", \"Dropoff Census Tract\")\n",
    "\n",
    "#if we decide not to use lat / long data\n",
    "\n",
    "#df = df.drop(\"Pickup Census Tract\", \"Dropoff Census Tract\",\"Pickup Centroid Latitude\",\n",
    "#             \"Pickup Centroid Longitude\",\"Dropoff Centroid Latitude\",\"Dropoff Centroid Longitude\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IMPUTATION  - Forward Fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Not sure what makes the most sense -- should we just drop fields with blank pickup community areas? Not sure how we would fill in a way that is accurate? \n",
    "* Was thinking 'Forward Fill' code listed below but sorting by time doesn't give us any insight to where they were picked up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last\n",
    "import sys\n",
    "\n",
    "# define the window\n",
    "window = Window.orderBy('Trip Start Timestamp')\\\n",
    "               .rowsBetween(-sys.maxsize, 0)\n",
    "\n",
    "# define the forward-filled column\n",
    "filled_column_temperature = last(df['Pickup Community Area'], ignorenulls=True).over(window)\n",
    "\n",
    "# do the fill \n",
    "df = df.withColumn('Pickup Community Area Filled',  filled_column_temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding a new column - Date with closest hour -- to match to weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------------------+-------------------------+--------------------------+------------------------------------+----------------------------+------------+\n",
      "|Trip ID                                 |Trip Start Timestamp  |Trip End Timestamp    |Trip Seconds|Trip Miles|Pickup Community Area|Dropoff Community Area|Fare|Tip|Additional Charges|Trip Total|Shared Trip Authorized|Trips Pooled|Pickup Centroid Latitude|Pickup Centroid Longitude|Pickup Centroid Location            |Dropoff Centroid Latitude|Dropoff Centroid Longitude|Dropoff Centroid Location           |Pickup Community Area Filled|dt_truncated|\n",
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------------------+-------------------------+--------------------------+------------------------------------+----------------------------+------------+\n",
      "|d3bd9d2e7d956a31dbe63553a39f10da5f081663|01/01/2019 01:00:00 AM|01/01/2019 01:00:00 AM|333         |1.5       |22                   |22                    |5.0 |0  |2.5               |7.5       |true                  |1           |41.9208017043           |-87.6945323419           |POINT (-87.6945323419 41.9208017043)|41.9227117587            |-87.7218357934            |POINT (-87.7218357934 41.9227117587)|22                          |null        |\n",
      "|fd12dad7d7450cadcca0a4e5b140b7d4e2c3bad9|01/01/2019 01:00:00 AM|01/01/2019 01:00:00 AM|685         |1.3       |8                    |32                    |5.0 |0  |2.5               |7.5       |false                 |1           |41.8925077809           |-87.6262149064           |POINT (-87.6262149064 41.8925077809)|41.8710158803            |-87.6314065252            |POINT (-87.6314065252 41.8710158803)|8                           |null        |\n",
      "|d3c8f33e85a08f8b1c0c372a117011667c07a24a|01/01/2019 01:00:00 AM|01/01/2019 01:00:00 AM|337         |1.2       |24                   |24                    |0.0 |0  |2.5               |2.5       |true                  |3           |41.906025969            |-87.6753116216           |POINT (-87.6753116216 41.906025969) |41.9067077917            |-87.6846859492            |POINT (-87.6846859492 41.9067077917)|24                          |null        |\n",
      "|fd15e48d0df6a483ce87ef3a1c4e0348e6f4ff30|01/01/2019 01:00:00 AM|01/01/2019 01:45:00 AM|2354        |18.3      |46                   |6                     |15.0|0  |2.5               |17.5      |true                  |2           |41.7412427285           |-87.551428197            |POINT (-87.551428197 41.7412427285) |41.9442266014            |-87.6559981815            |POINT (-87.6559981815 41.9442266014)|46                          |null        |\n",
      "|d3d2cc01cb9a40dc4cc06507a6c4c5222ceec1c8|01/01/2019 01:00:00 AM|01/01/2019 01:00:00 AM|382         |1.2       |6                    |7                     |5.0 |0  |2.5               |7.5       |false                 |1           |41.9363101308           |-87.6515625922           |POINT (-87.6515625922 41.9363101308)|41.9218549112            |-87.6462109769            |POINT (-87.6462109769 41.9218549112)|6                           |null        |\n",
      "|fd1c2f16b2c3c22e69a9b1510b7a5bab5d475023|01/01/2019 01:00:00 AM|01/01/2019 01:15:00 AM|737         |2.4       |32                   |8                     |7.5 |0  |2.5               |10.0      |false                 |1           |41.8774061234           |-87.6219716519           |POINT (-87.6219716519 41.8774061234)|41.8991556134            |-87.6262105324            |POINT (-87.6262105324 41.8991556134)|32                          |null        |\n",
      "|d3d46d123e46fc743017cb790ff85765869533b5|01/01/2019 01:00:00 AM|01/01/2019 01:15:00 AM|972         |1.0       |8                    |32                    |10.0|0  |2.8               |12.8      |false                 |1           |41.8920421365           |-87.6318639497           |POINT (-87.6318639497 41.8920421365)|41.8849871918            |-87.6209929134            |POINT (-87.6209929134 41.8849871918)|8                           |null        |\n",
      "|fd2bfa431d629227c9895b484c359f68d621fb0c|01/01/2019 01:00:00 AM|01/01/2019 01:15:00 AM|544         |2.2       |8                    |7                     |5.0 |0  |2.5               |7.5       |false                 |1           |41.9074128162           |-87.6409015248           |POINT (-87.6409015248 41.9074128162)|41.9287630064            |-87.665676837             |POINT (-87.665676837 41.9287630064) |8                           |null        |\n",
      "|d3d545d884513af1d5e20c481d7513eea4b41566|01/01/2019 01:00:00 AM|01/01/2019 01:15:00 AM|865         |3.1       |8                    |33                    |7.5 |0  |2.5               |10.0      |false                 |1           |41.8919715078           |-87.6129454143           |POINT (-87.6129454143 41.8919715078)|41.859349715             |-87.6173580061            |POINT (-87.6173580061 41.859349715) |8                           |null        |\n",
      "|fd2fbc6a5e8e92d1e2627dc8330532697a7aa84b|01/01/2019 01:00:00 AM|01/01/2019 01:15:00 AM|962         |2.8       |7                    |22                    |7.5 |0  |2.5               |10.0      |false                 |1           |41.9289672664           |-87.6561568309           |POINT (-87.6561568309 41.9289672664)|41.9211259143            |-87.6997544065            |POINT (-87.6997544065 41.9211259143)|7                           |null        |\n",
      "+----------------------------------------+----------------------+----------------------+------------+----------+---------------------+----------------------+----+---+------------------+----------+----------------------+------------+------------------------+-------------------------+------------------------------------+-------------------------+--------------------------+------------------------------------+----------------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, unix_timestamp, round\n",
    "\n",
    "dt_truncated = ((round(unix_timestamp(col(\"Trip Start Timestamp\")) / 12) * 12)\n",
    "    .cast(\"timestamp\"))\n",
    "\n",
    "df.withColumn(\"dt_truncated\", dt_truncated).show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD IMPUTATION ATTEMPS (IGNORE) - Getting errors due to data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "toImpute = df.select(\"Pickup Community Area\",\"Dropoff Community Area\",\"Fare\",\n",
    "                           \"Additional Charges\",\"Trip Total\")\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=df.columns, \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in toImpute.columns]\n",
    ")\n",
    "imputer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer=Imputer(inputCols=[\"Pickup Community Area\",\"Dropoff Community Area\",\"Fare\",\n",
    "                           \"Additional Charges\",\"Trip Total\"],\n",
    "                outputCols=[\"Pickup Community Area\",\"Dropoff Community Area\",\"Fare\",\n",
    "                           \"Additional Charges\",\"Trip Total\"])\n",
    "model=imputer.fit(df)\n",
    "df=model.transform(df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into train / test\n",
    "\n",
    "splits = df_combine.randomSplit([0.8, 0.2])\n",
    "df_train = splits[0]\n",
    "df_test = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features_norm', labelCol = 'label', maxIter=10, regParam=0.3, \n",
    "                        elasticNetParam=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "model = pipeline.fit(df_train)\n",
    "prediction = model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate test data\n",
    "prediction = model.transform(df_test)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\").setLabelCol(\"label\")\n",
    "    \n",
    "binEval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /home/abertin/data/\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder.appName(\"TrafficCrashes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /home/abertin/data/Traffic_Crashes.csv /user/abertin/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1-Instacart-AssociationMining.ipynb\tinstacart\r\n",
      "BigData_Assignment3_AlisonBertin.ipynb\tmobydick.txt\r\n",
      "BigData_Project.ipynb\t\t\tTraffic_Crashes.csv\r\n",
      "Crimes2001_to_present.csv\t\twordcount.sh\r\n",
      "food-inspections.csv\r\n"
     ]
    }
   ],
   "source": [
    "df = sc.textFile(\"/user/abertin/data/Traffic_Crashes.csv\").map(lambda line: line.split(\",\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 4G 32e",
   "language": "python",
   "name": "pyspark2_4g32e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
