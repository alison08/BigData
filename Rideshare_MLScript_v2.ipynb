{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col, lit, udf, month, year, date_format, datediff, from_unixtime, unix_timestamp\n",
    "from pyspark.sql.functions import date_trunc\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (StructType, StructField, DateType, BooleanType,\n",
    "                               DoubleType, IntegerType, StringType, TimestampType)\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=512M; support was removed in 8.0\n",
      "Found 4 items\n",
      "-rw-r--r--   3 alphan alphan 1824926642 2019-05-25 04:44 /user/alphan/data/chicago_crimes.csv\n",
      "drwxr-xr-x   - alphan alphan          0 2019-06-04 02:09 /user/alphan/data/df.csv\n",
      "drwxr-xr-x   - alphan alphan          0 2019-06-04 02:12 /user/alphan/data/final_project_df.csv\n",
      "-rw-r--r--   3 alphan alphan  208276005 2019-04-30 20:10 /user/alphan/data/food-inspections.csv\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -ls /user/mechols/data/\n",
    "!hdfs dfs -ls /user/alphan/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('RideShare').getOrCreate()\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '15g'), ('spark.app.name', 'Spark Updated Conf'), ('spark.executor.cores', '4'), ('spark.cores.max', '4'), ('spark.driver.memory','20g')])\n",
    "#df = spark.read.csv(\"/user/mechols/data/fulldf.csv\", inferSchema=True, header=True)\n",
    "df = spark.read.csv(\"/user/alphan/data/final_project_df.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('RideShare').getOrCreate()\n",
    "conf = spark.sparkContext._conf.setAll([('spark.executor.memory', '256g'),\n",
    "                                        ('spark.app.name', 'Spark Updated Conf'),\n",
    "                                        ('spark.executor.cores', '16'),\n",
    "                                        ('spark.cores.max', '16'),\n",
    "                                        ('spark.driver.memory','256g'),\n",
    "                                        ('spark.sql.AutoBroadcastJoinThreshold', -1),\n",
    "                                        ('mapreduce.reduce.memory.mb',-1),\n",
    "                                        ('spark.yarn.executor.memoryOverhead', -1),\n",
    "                                        ('spark.kryoserializer.buffer.max.mb', '5g')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE to HDFS\n",
    "#fulldf = rides.join(spark_weather, rides.startTime == spark_weather.time, how='inner')\n",
    "\n",
    "#res_path = '/user/alphan/data/final_project_df.csv'\n",
    "#df.write.csv(path=res_path, header=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is there a linear correlation between columns\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "standardscaler=StandardScaler().setInputCol(\"features\").setOutputCol(\"Scaled_features\")\n",
    "raw_data=standardscaler.fit(raw_data).transform(raw_data)\n",
    "raw_data.select(\"features\",\"Scaled_features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[430.0,3.0,0.0,3....|\n",
      "|[368.0,1.9,1.0,44...|\n",
      "|[1142.0,14.7,1.0,...|\n",
      "|[1288.0,3.9,1.0,4...|\n",
      "|[205.0,1.2,0.0,10...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Assemble vectors and Scale:\n",
    "\n",
    "columns = ['seconds','miles','shared','communityPickup','communityDropoff','humidity', \n",
    "'apparentTemperature','precipIntensity',\n",
    "'precipProbability', 'temperature', 'Cloudy','Rainy', 'Snowy','month','day', 'hour']\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler,StandardScaler\n",
    "vectorAssembler = VectorAssembler(inputCols = columns, outputCol = 'features')# 'fare', 'addCharge', 'tripTotal'\n",
    "ml_data=vectorAssembler.transform(df)\n",
    "ml_data.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            features|     scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[430.0,3.0,0.0,3....|[0.65134636695970...|\n",
      "|[368.0,1.9,1.0,44...|[0.55743130939807...|\n",
      "|[1142.0,14.7,1.0,...|[1.72985476992555...|\n",
      "|[1288.0,3.9,1.0,4...|[1.95100958289326...|\n",
      "|[205.0,1.2,0.0,10...|[0.31052559355055...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "standardscaler=StandardScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "scaled_data=standardscaler.fit(ml_data).transform(ml_data)\n",
    "scaled_data.select('features','scaled_features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|     scaled_features|fare|\n",
      "+--------------------+----+\n",
      "|[0.65134636695970...| 7.5|\n",
      "|[0.55743130939807...| 5.0|\n",
      "|[1.72985476992555...|17.5|\n",
      "+--------------------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#stages = [vector, scaler, ]\n",
    "#pipe = Pipeline(stages=stages)\n",
    "\n",
    "vfull_df = scaled_data.select(['scaled_features', 'fare'])\n",
    "vfull_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data\n",
    "splits = scaled_data.randomSplit([0.99, 0.01])\n",
    "small_df = splits[1]\n",
    "small_split = small_df.randomSplit([0.7, 0.3])\n",
    "train_df = small_split[0]\n",
    "test_df = small_split[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.39705\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "dt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'fare')\n",
    "dt_model = dt.fit(train_df)\n",
    "dt_predictions = dt_model.transform(test_df)\n",
    "dt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"fare\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = dt_evaluator.evaluate(dt_predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from FeatureImportanceSelector import ExtractFeatureImp, FeatureImpSelector\n",
    "ExtractFeatureImp(mod.stages[-1].featureImportances, dt_predictions, \"features_subset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.DataFrame(dt_model.featureImportances.toArray(), columns=[\"values\"])\n",
    "features_col = pd.Series(features)\n",
    "model[\"features\"] = features_col\n",
    "model.sort_values(\"values\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+--------------------+\n",
      "|        prediction|fare|            features|\n",
      "+------------------+----+--------------------+\n",
      "| 3.191120111559453| 5.0|[35.0,0.1,0.0,61....|\n",
      "|3.2407646102402516| 5.0|[48.0,0.3,1.0,28....|\n",
      "|3.1876104336031577| 2.5|[63.0,0.1,0.0,39....|\n",
      "|3.0995382207737494| 2.5|[73.0,0.1,0.0,33....|\n",
      "|3.1183621383751605| 2.5|[82.0,0.3,0.0,1.0...|\n",
      "+------------------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'fare', maxIter=10)\n",
    "gbt_model = gbt.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_predictions.select('prediction', 'fare', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.14563\n",
      "Root Mean Squared Error (RMSE) on test data = 0.758795\n"
     ]
    }
   ],
   "source": [
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"fare\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "\n",
    "gbt_evaluator2 = RegressionEvaluator(\n",
    "    labelCol=\"fare\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "gbt_r2 = gbt_evaluator2.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 2.94831\n",
      "Root Mean Squared Error (RMSE) on test data = 0.782302\n"
     ]
    }
   ],
   "source": [
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % gbt_rmse)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % gbt_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.DataFrame(gbt_model.featureImportances.toArray(), columns=[\"values\"])\n",
    "features_col = pd.Series(features)\n",
    "model[\"features\"] = features_col\n",
    "model.sort_values(\"values\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+--------------------+\n",
      "|       prediction|fare|            features|\n",
      "+-----------------+----+--------------------+\n",
      "|4.931155508072363| 2.5|[39.0,0.2,0.0,3.0...|\n",
      "|4.869704038876839| 5.0|[50.0,0.1,1.0,16....|\n",
      "|4.935776284030992| 2.5|[64.0,0.2,0.0,8.0...|\n",
      "|4.833982624632548| 2.5|[73.0,0.3,1.0,60....|\n",
      "|4.864347998313905| 2.5|[84.0,0.2,0.0,32....|\n",
      "+-----------------+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol = 'features', labelCol = 'fare')\n",
    "rfModel = rf.fit(train_df)\n",
    "rf_predictions = rfModel.transform(test_df)\n",
    "rf_predictions.select('prediction', 'fare', 'features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.14563\n",
      "Root Mean Squared Error (RMSE) on test data = 0.758795\n"
     ]
    }
   ],
   "source": [
    "rf_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"fare\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = rf_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "rf_evaluator2 = RegressionEvaluator(\n",
    "    labelCol=\"fare\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = rf_evaluator2.evaluate(rf_predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.DataFrame(rfModel.featureImportances.toArray(), columns=[\"values\"])\n",
    "features_col = pd.Series(features)\n",
    "model[\"features\"] = features_col\n",
    "model.sort_values(\"values\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0019992517787194124,0.9857414716629734,-3.3667961022972794,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]\n",
      "Intercept: 3.7666711227798704\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='fare', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.297317\n",
      "r2: 0.765130\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models with Gradient Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- seconds: integer (nullable = true)\n",
      " |-- miles: double (nullable = true)\n",
      " |-- shared: boolean (nullable = true)\n",
      " |-- communityPickup: integer (nullable = true)\n",
      " |-- communityDropoff: integer (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- apparentTemperature: double (nullable = true)\n",
      " |-- precipIntensity: double (nullable = true)\n",
      " |-- precipProbability: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- Cloudy: double (nullable = true)\n",
      " |-- Rainy: double (nullable = true)\n",
      " |-- Snowy: double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_data = train_df.select(col(\"fare\").alias(\"label\"), *columns)  \n",
    "lr_data.printSchema()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "rfr = RandomForestRegressor(labelCol=\"label\", featuresCol=\"scaled_features\")\n",
    "stages = [vectorAssembler, standardscaler, rfr]\n",
    "pipe = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorParam = ParamGridBuilder() \\\n",
    ".addGrid(rfr.maxDepth, [4, 6]) \\\n",
    ".addGrid(rfr.maxBins, [5, 10]) \\\n",
    ".addGrid(rfr.impurity, [\"variance\"]) \\\n",
    ".build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipe,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)\n",
    "\n",
    "cvmodel = crossval.fit(lr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEvaluator=RegressionEvaluator()\n",
    "eval_rmse = RegressionEvaluator(metricName=\"rmse\")\n",
    "eval_r2 = RegressionEvaluator(metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2565578698190656\n",
      "0.7710192256352868\n"
     ]
    }
   ],
   "source": [
    "#Not sure it matters what data we use here\n",
    "print(eval_rmse.evaluate(cvmodel.transform(lr_data)))\n",
    "print(eval_r2.evaluate(cvmodel.transform(lr_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(labelCol=\"label\", featuresCol=\"scaled_features\")\n",
    "stages = [vectorAssembler, standardscaler, gbt]\n",
    "pipe = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimatorParam = ParamGridBuilder() \\\n",
    ".addGrid(rfr.maxDepth, [4, 6]) \\\n",
    ".addGrid(rfr.maxBins, [5, 10]) \\\n",
    ".addGrid(rfr.impurity, [\"variance\"]) \\\n",
    ".build()\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipe,\n",
    "                         estimatorParamMaps=estimatorParam,\n",
    "                         evaluator=evaluator,\n",
    "                         numFolds=3)\n",
    "\n",
    "cvmodel = crossval.fit(lr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelEvaluator=RegressionEvaluator()\n",
    "eval_rmse = RegressionEvaluator(metricName=\"rmse\")\n",
    "eval_r2 = RegressionEvaluator(metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.009552922333908\n",
      "0.8058905579571947\n"
     ]
    }
   ],
   "source": [
    "print(eval_rmse.evaluate(cvmodel.transform(lr_data)))\n",
    "print(eval_r2.evaluate(cvmodel.transform(lr_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seconds: integer (nullable = true)\n",
      " |-- miles: double (nullable = true)\n",
      " |-- communityPickup: integer (nullable = true)\n",
      " |-- communityDropoff: integer (nullable = true)\n",
      " |-- fare: double (nullable = true)\n",
      " |-- shared: boolean (nullable = true)\n",
      " |-- pickupLat: double (nullable = true)\n",
      " |-- pickupLong: double (nullable = true)\n",
      " |-- dropoffLat: double (nullable = true)\n",
      " |-- dropoffLong: double (nullable = true)\n",
      " |-- apparentTemperature: double (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- precipIntensity: double (nullable = true)\n",
      " |-- precipProbability: double (nullable = true)\n",
      " |-- temperature: double (nullable = true)\n",
      " |-- Cloudy: double (nullable = true)\n",
      " |-- Rainy: double (nullable = true)\n",
      " |-- Snowy: double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 4G 32e",
   "language": "python",
   "name": "pyspark2_4g32e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
